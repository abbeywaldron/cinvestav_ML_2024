{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN_parabola  - This is a Multilayer Perceptron (MLP) example using Keras\n",
    "\n",
    "    Copyright (C) 2020 Adrian Bevan, and 2023, 2024 Abbey Waldron\n",
    "    Queen Mary University of London\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the GNU General Public License\n",
    "    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n",
    "    \n",
    "----------------------\n",
    "\n",
    "## The problem\n",
    "\n",
    "Use an MLP to approximate the function $y=x^2$ to illustrate the ability of this algorithm to be a function approximator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "Ntrain = ...\n",
    "Ntest  = 100\n",
    "xmin   =-10\n",
    "xmax   = 10\n",
    "Noise  = 0.05\n",
    "print(\"\\033[92mGenerating the parabola data set\\033[0m\")\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test  = []\n",
    "Y_test  = []\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "def sim_parabola(xmin, xmax, Noise):\n",
    "    \"\"\"\n",
    "    Function to simulate a random data point for a parabola\n",
    "    \"\"\"\n",
    "    x = np.random.random()*(xmax-xmin)+xmin\n",
    "    y = x*x*(1+np.random.random()*Noise)\n",
    "    \n",
    "    return x, y\n",
    "#--------------------------------------------------------------------\n",
    "  \n",
    "for i in range( Ntrain ):\n",
    "  x,y = sim_parabola(xmin, xmax, Noise)\n",
    "  X_train.append(x)\n",
    "  Y_train.append(y)\n",
    "\n",
    "for i in range( Ntest ):\n",
    "  x,y = ...\n",
    "  X_test.append(...)\n",
    "  Y_test.append(...)\n",
    "\n",
    "# convert to nparrays\n",
    "x_test  = np.array(X_test)\n",
    "y_test  = ...\n",
    "x_train = ...\n",
    "y_train = np.array(Y_train)\n",
    "\n",
    "print(\"Have generated the following data:\")\n",
    "print(\"\\tN(test)  = \", len(x_test))\n",
    "print(\"\\tN(train) = \", len(x_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Building the model \n",
    "\n",
    "**Training [Epochs, batches and validation data]** The number of training epochs specified is denoted by `Nepochs`.  1 epoch is required to run over all of the training data.  It is possible to run batches or mini-batches of data through the training; each batch requires the optimisation to be performed, and so when specifying the `BatchSize` the training will be performed by running the optimisation $N_{epochs}\\times N_{batches}$ times.  In general this leads to faster optimisation of the model than optimising over the full training set each time.\n",
    "\n",
    "The training data will be split into training and validation samples according to the value of the variable `ValidationSplit`. \n",
    "\n",
    "**Dropout:** Coadaptation is the ability for an optimisation algorithm to allow weights to be learned where changes in one node can be compensated by changes in another node that limit the increase in performance.  This issue can be a problem for deep networks in particular where the optimisation process can involve millions of hyperparameters.  A way to combat this issue is to randomly drop-out nodes in the network each iteration of the optimisation.  That way no single paring of nodes can learn to co-adapt to the evolution of hyperparameters through the optimisation.  Thankfully all the user has to do is to set a dropout value via the variable `DropoutValue`. \n",
    "\n",
    "**NOTE:** this value is the fraction of nodes dropped from the model.\n",
    "\n",
    "**Loss:** The cross entropy loss function is used for this optimisation process.  The value of the loss function is converted into an output vector of 1's and 0's to be used for classification.\n",
    "\n",
    "### Model configuration\n",
    "\n",
    "The MLP consists of an input layer, two hidden layers, in this case one drop out layer and finally an output layer. These are as follows:\n",
    "- **Inputs:** This model specifies an input shape of 1.\n",
    "- **Hidden Layers:** This network has two hidden layers, wich using a leaky ReLU (Rectified Linear Unit) activation function.  The parameter alpha defines the slope of the function for negative values, and for positive values the activation function is simply $f(x)=x$.\n",
    "- **Dropout Layer:** Here the only dropout layer is the one from the second hidden layer of the network to the output.  You may wish to explore what happens if another dropout layer is added after the first hidden layer.\n",
    "- **Output:** The output is 1 number. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training configuration\n",
    "#\n",
    "ValidationSplit = 0.5\n",
    "BatchSize       = 100\n",
    "Nepochs         = ...\n",
    "DropoutValue    = 0.2\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(128, input_dim=1, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n",
    "  tf.keras.layers.Dropout(DropoutValue),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[92mWill train a multilayer perceptron using some toy data following y = x^2\\033[0m\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"2 layer MLP with configuration 1:128:128:1\")\n",
    "print(\"Dropout values       = \", DropoutValue)\n",
    "print(\"Leaky relu parameter =  0.1\")\n",
    "print(\"ValidationSplit      = \", ValidationSplit)\n",
    "print(\"BatchSize            = \", BatchSize)\n",
    "print(\"Nepochs              = \", Nepochs)\n",
    "\n",
    "# now specify the loss function \n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "learning_rate = 0.001 # alpha.\n",
    "\n",
    "# now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "# the fit step specifies the number of training epochs\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=loss_fn)\n",
    "history  = model.fit(x_train, y_train, validation_split=ValidationSplit, batch_size=BatchSize, epochs=Nepochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "## Output\n",
    "\n",
    "When training a model we are interested in studying the accuracy of prediction (in this case how close is the predicted model, $\\widehat{y}$ value to the function that we are trying to approximate, $x^2$), and about the evolution of the loss function for both the test and train samples of examples.  If the test and train sample loss functions are signficantly different that points toward a problem... the model could be under or over trained.\n",
    "\n",
    "This information is stored in the 'history' variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"\\033[1mDisplay the evolution of the loss as a function of the training epoch\\033[0m\\n\")\n",
    "print(\"  N(Epochs)        = \", Nepochs)\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(...) # plot the 'loss'\n",
    "plt.plot(...) # plot the 'val_loss'\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# having finished training the model, use this to evaluate the performance on an\n",
    "# independent sample of test data\n",
    "loss = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(\"loss = {:5.3f}\".format(loss))\n",
    "\n",
    "print(\"\\n\\033[1mDisplay the model prediction against the ground truth from test data\\033[0m\\n\")\n",
    "\n",
    "#\n",
    "# use the model to make predictions based on the unseen test data\n",
    "#\n",
    "y_predict = model.predict(...) # run the prediction on x_test\n",
    "plt.plot(x_test, y_predict, \"r.\")\n",
    "plt.plot(..., ..., \"b.\") # plot the original test data, x_test and y_test\n",
    "plt.title('model prediction')\n",
    "plt.ylabel('$y=x^{2}$')\n",
    "plt.xlabel('$x$')\n",
    "plt.legend(['model', 'ground truth'], loc='lower right')\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "print(\"\\n\\033[1mDisplay the difference between the model prediction and the ground truth from test data\\033[0m\\n\")\n",
    "\n",
    "delta = []\n",
    "deltapc = []\n",
    "for i in range(len(y_predict)):\n",
    "    thedelta = y_predict[i]-y_test[i]\n",
    "    delta.append( thedelta )\n",
    "    if( x_test[i] ):\n",
    "       deltapc.append( thedelta /  x_test[i] )\n",
    "    else:\n",
    "       deltapc.append( 0.0 )\n",
    "    \n",
    "plt.plot(x_test, delta, \"b.\")\n",
    "plt.plot(x_test, deltapc, \"r.\")\n",
    "plt.legend(['$\\Delta_y$', '$\\Delta_y$ (fraction)'], loc='upper right')\n",
    "plt.title('model prediction accuracy')\n",
    "plt.ylabel('$\\widehat{y}-y$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylim(-20, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Exercises:\n",
    "\n",
    " - Explore the effect of DropOut, ValidationSplit, Nepochs, and BatchSize have on the training (try to find a model where the test and train loss function values are similar.\n",
    " - Explore how the neural network structure affects the training performance (e.g. add double or halve the number of nodes in the hidden layers, the current value is 128 for both)\n",
    " - Explore the effect of adding a second dropout layer into the network after the first hidden layer.\n",
    " - Explore what happens when the model is reduced to a single layer perceptron (removing the second hidden layer).\n",
    " - Explore what happens when the model is changed by adding a third hidden layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
